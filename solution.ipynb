{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb9b8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install implicit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "from catboost import CatBoostRanker, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# ============ КОНСТАНТЫ И КОНФИГУРАЦИЯ ============\n",
    "\n",
    "class Constants:\n",
    "    \"\"\"Класс с константами для имен колонок и значений\"\"\"\n",
    "    # Имена файлов\n",
    "    TRAIN_FILE = \"train.csv\"\n",
    "    CANDIDATES_FILE = \"candidates.csv\"\n",
    "    USERS_FILE = \"users.csv\"\n",
    "    BOOKS_FILE = \"books.csv\"\n",
    "    GENRES_FILE = \"genres.csv\"\n",
    "    BOOK_GENRES_FILE = \"book_genres.csv\"\n",
    "    BOOK_DESCRIPTIONS_FILE = \"book_descriptions.csv\"\n",
    "    \n",
    "    # Имена колонок\n",
    "    COL_USER_ID = \"user_id\"\n",
    "    COL_BOOK_ID = \"book_id\"\n",
    "    COL_TIMESTAMP = \"timestamp\"\n",
    "    COL_HAS_READ = \"has_read\"\n",
    "    COL_RATING = \"rating\"\n",
    "    COL_TARGET = \"target\"\n",
    "    \n",
    "    # Метаданные пользователей\n",
    "    COL_GENDER = \"gender\"\n",
    "    COL_AGE = \"age\"\n",
    "    \n",
    "    # Метаданные книг\n",
    "    COL_AUTHOR_ID = \"author_id\"\n",
    "    COL_PUBLISHER = \"publisher\"\n",
    "    COL_LANGUAGE = \"language\"\n",
    "    COL_TITLE = \"title\"\n",
    "    COL_AUTHOR_NAME = \"author_name\"\n",
    "    COL_PUBLICATION_YEAR = \"publication_year\"\n",
    "    COL_IMAGE_URL = \"image_url\"\n",
    "    COL_DESCRIPTION = \"description\"\n",
    "    COL_GENRE_ID = \"genre_id\"\n",
    "    COL_GENRE_NAME = \"genre_name\"\n",
    "    \n",
    "    # Значения для целевой переменной\n",
    "    TARGET_READ = 2      # Прочитал\n",
    "    TARGET_PLANNED = 1   # Планирует прочитать\n",
    "    TARGET_NEGATIVE = 0  # Негативный пример\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Класс конфигурации с параметрами проекта\"\"\"\n",
    "    # --- ПУТИ ---\n",
    "    ROOT_DIR = Path(\".\")\n",
    "    DATA_DIR = ROOT_DIR / \"/kaggle/input/nto-team-tour\" / \"public\" # ЗАМЕНИТЕ НА СВОЮ ССЫЛКУ\n",
    "    MODEL_DIR = ROOT_DIR / \"models\"\n",
    "    RESULTS_DIR = ROOT_DIR / \"results\"\n",
    "    \n",
    "    # Создание директорий\n",
    "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # --- ПАРАМЕТРЫ ---\n",
    "    RANDOM_STATE = 42\n",
    "    VAL_SPLIT_SIZE = 0.1  # Размер валидационной выборки\n",
    "    TEMPORAL_VAL_DAYS = 30  # Дней для временного сплита\n",
    "    \n",
    "    # --- ALS ПАРАМЕТРЫ ---\n",
    "    ALS_FACTORS = 64\n",
    "    ALS_REGULARIZATION = 0.05\n",
    "    ALS_ITERATIONS = 40\n",
    "    ALS_TOP_K_CANDIDATES = 50\n",
    "    \n",
    "    # Веса взаимодействий для ALS\n",
    "    ALS_WEIGHT_READ = 4      # Прочитал\n",
    "    ALS_WEIGHT_PLANNED = 1   # Планирует\n",
    "    \n",
    "    # --- CatBoost ПАРАМЕТРЫ ---\n",
    "    CB_ITERATIONS = 5000\n",
    "    CB_LEARNING_RATE = 0.02\n",
    "    CB_DEPTH = 10\n",
    "    CB_L2_LEAF_REG = 10\n",
    "    CB_EVAL_METRIC = \"NDCG:top=20\"\n",
    "    CB_EARLY_STOPPING_ROUNDS = 150\n",
    "    \n",
    "    # --- TF-IDF ПАРАМЕТРЫ ---\n",
    "    TFIDF_MAX_FEATURES = 1000\n",
    "    TFIDF_SVD_COMPONENTS = 400\n",
    "    \n",
    "    # --- ФИЧИ ---\n",
    "    CATEGORICAL_FEATURES = [\n",
    "        Constants.COL_GENDER,\n",
    "        Constants.COL_AUTHOR_ID,\n",
    "        Constants.COL_PUBLISHER,\n",
    "        Constants.COL_LANGUAGE\n",
    "    ]\n",
    "    \n",
    "    TEXT_FEATURES_TO_DROP = [\n",
    "        Constants.COL_TITLE,\n",
    "        Constants.COL_AUTHOR_NAME,\n",
    "        Constants.COL_DESCRIPTION,\n",
    "        Constants.COL_GENRE_NAME\n",
    "    ]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_cb_params(cls):\n",
    "        \"\"\"Возвращает параметры для CatBoost\"\"\"\n",
    "        return {\n",
    "            'loss_function': 'YetiRank',\n",
    "            'iterations': cls.CB_ITERATIONS,\n",
    "            'learning_rate': cls.CB_LEARNING_RATE,\n",
    "            'depth': cls.CB_DEPTH,\n",
    "            'l2_leaf_reg': cls.CB_L2_LEAF_REG,\n",
    "            'random_seed': cls.RANDOM_STATE,\n",
    "            'eval_metric': cls.CB_EVAL_METRIC,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': cls.CB_EARLY_STOPPING_ROUNDS,\n",
    "            'task_type':\"GPU\"\n",
    "        }\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"Установка сидов для воспроизводимости\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "# ============ УТИЛИТЫ ============\n",
    "\n",
    "class DataOptimizer:\n",
    "    \"\"\"Класс для оптимизации памяти в данных\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def reduce_mem_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Итеративно изменяет типы данных для уменьшения использования памяти\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame для оптимизации\n",
    "            \n",
    "        Returns:\n",
    "            Оптимизированный DataFrame\n",
    "        \"\"\"\n",
    "        start_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f\"Начальное использование памяти: {start_mem:.2f} MB\")\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            \n",
    "            # Пропускаем нечисловые типы и даты\n",
    "            if (col_type != object and \n",
    "                not str(col_type).startswith('datetime') and\n",
    "                col_type.name != 'category'):\n",
    "                \n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                \n",
    "                # Оптимизация целых чисел\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                # Оптимизация чисел с плавающей точкой\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Конечное использование памяти: {end_mem:.2f} MB\")\n",
    "        print(f\"Сокращение на {reduction:.1f}%\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_and_optimize(filepath: Path, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Загружает и оптимизирует DataFrame\n",
    "        \n",
    "        Args:\n",
    "            filepath: Путь к файлу\n",
    "            **kwargs: Дополнительные аргументы для pd.read_csv\n",
    "            \n",
    "        Returns:\n",
    "            Оптимизированный DataFrame\n",
    "        \"\"\"\n",
    "        print(f\"Загрузка {filepath.name}...\")\n",
    "        df = pd.read_csv(filepath, **kwargs)\n",
    "        df = DataOptimizer.reduce_mem_usage(df)\n",
    "        return df\n",
    "\n",
    "\n",
    "# ============ ЗАГРУЗКА ДАННЫХ ============\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Класс для загрузки и подготовки данных\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.constants = Constants()\n",
    "    \n",
    "    def load_all_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Загружает все необходимые данные\n",
    "        \n",
    "        Returns:\n",
    "            Словарь с DataFrame\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        \n",
    "        # Загрузка тренировочных данных\n",
    "        data['train'] = DataOptimizer.load_and_optimize(\n",
    "            self.config.DATA_DIR / self.constants.TRAIN_FILE,\n",
    "            parse_dates=[self.constants.COL_TIMESTAMP]\n",
    "        )\n",
    "        \n",
    "        # Загрузка кандидатов\n",
    "        data['candidates'] = DataOptimizer.load_and_optimize(\n",
    "            self.config.DATA_DIR / self.constants.CANDIDATES_FILE\n",
    "        )\n",
    "        \n",
    "        # Загрузка метаданных пользователей\n",
    "        data['users'] = DataOptimizer.load_and_optimize(\n",
    "            self.config.DATA_DIR / self.constants.USERS_FILE\n",
    "        )\n",
    "        \n",
    "        # Загрузка метаданных книг\n",
    "        data['books'] = DataOptimizer.load_and_optimize(\n",
    "            self.config.DATA_DIR / self.constants.BOOKS_FILE\n",
    "        )\n",
    "        \n",
    "        # Загрузка описаний книг\n",
    "        try:\n",
    "            data['book_descriptions'] = DataOptimizer.load_and_optimize(\n",
    "                self.config.DATA_DIR / self.constants.BOOK_DESCRIPTIONS_FILE\n",
    "            )\n",
    "            print(f\"Book descriptions shape: {data['book_descriptions'].shape}\")\n",
    "        except:\n",
    "            print(\"Book descriptions file not found, skipping...\")\n",
    "            data['book_descriptions'] = pd.DataFrame()\n",
    "        \n",
    "        # Загрузка жанров\n",
    "        try:\n",
    "            data['genres'] = DataOptimizer.load_and_optimize(\n",
    "                self.config.DATA_DIR / self.constants.GENRES_FILE\n",
    "            )\n",
    "            data['book_genres'] = DataOptimizer.load_and_optimize(\n",
    "                self.config.DATA_DIR / self.constants.BOOK_GENRES_FILE\n",
    "            )\n",
    "            print(f\"Genres shape: {data['genres'].shape}\")\n",
    "            print(f\"Book genres shape: {data['book_genres'].shape}\")\n",
    "        except:\n",
    "            print(\"Genres files not found, skipping...\")\n",
    "            data['genres'] = pd.DataFrame()\n",
    "            data['book_genres'] = pd.DataFrame()\n",
    "        \n",
    "        print(f\"Train shape: {data['train'].shape}\")\n",
    "        print(f\"Candidates shape: {data['candidates'].shape}\")\n",
    "        print(f\"Users shape: {data['users'].shape}\")\n",
    "        print(f\"Books shape: {data['books'].shape}\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "    \n",
    "    \n",
    "    def prepare_target(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Подготавливает целевую переменную для ранжирования\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame с взаимодействиями\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame с подготовленной целевой переменной\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Создаем целевую переменную с градациями\n",
    "        df[self.constants.COL_TARGET] = df[self.constants.COL_HAS_READ].apply(\n",
    "            lambda x: self.constants.TARGET_READ if x == 1 \n",
    "                     else self.constants.TARGET_PLANNED\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "# ============ ALS МОДЕЛЬ ============\n",
    "\n",
    "class ALSRecommender:\n",
    "    \"\"\"Класс для генерации кандидатов с помощью ALS\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.user2idx = None\n",
    "        self.item2idx = None\n",
    "        self.idx2item = None\n",
    "        self.sparse_matrix = None\n",
    "    \n",
    "    def prepare_sparse_matrix(self, df: pd.DataFrame) -> sparse.csr_matrix:\n",
    "        \"\"\"\n",
    "        Подготавливает разреженную матрицу для ALS\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame с взаимодействиями\n",
    "            \n",
    "        Returns:\n",
    "            Разреженная матрица взаимодействий\n",
    "        \"\"\"\n",
    "        # Создаем маппинги\n",
    "        unique_users = df[Constants.COL_USER_ID].unique()\n",
    "        unique_items = df[Constants.COL_BOOK_ID].unique()\n",
    "        \n",
    "        self.user2idx = {u: i for i, u in enumerate(unique_users)}\n",
    "        self.item2idx = {i: idx for idx, i in enumerate(unique_items)}\n",
    "        self.idx2item = {idx: i for i, idx in self.item2idx.items()}\n",
    "        \n",
    "        # Создаем веса взаимодействий (читал = 4, планирует = 1)\n",
    "        df['weight'] = df[Constants.COL_HAS_READ].apply(\n",
    "            lambda x: 4 if x == 1 else 1\n",
    "        )\n",
    "        \n",
    "        # Создаем разреженную матрицу\n",
    "        rows = df[Constants.COL_USER_ID].map(self.user2idx).values\n",
    "        cols = df[Constants.COL_BOOK_ID].map(self.item2idx).values\n",
    "        data = df['weight'].values\n",
    "        \n",
    "        sparse_matrix = sparse.csr_matrix(\n",
    "            (data, (rows, cols)), \n",
    "            shape=(len(unique_users), len(unique_items))\n",
    "        )\n",
    "        \n",
    "        return sparse_matrix\n",
    "    \n",
    "    def train(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Обучает ALS модель\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame с взаимодействиями\n",
    "        \"\"\"\n",
    "        print(\"Обучение ALS модели...\")\n",
    "        self.sparse_matrix = self.prepare_sparse_matrix(df)\n",
    "        \n",
    "        self.model = implicit.als.AlternatingLeastSquares(\n",
    "            factors=self.config.ALS_FACTORS,\n",
    "            regularization=self.config.ALS_REGULARIZATION,\n",
    "            iterations=self.config.ALS_ITERATIONS,\n",
    "            random_state=self.config.RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.sparse_matrix)\n",
    "        print(\"ALS модель обучена\")\n",
    "    \n",
    "    def generate_candidates(self, df: pd.DataFrame, users_to_predict: List[int], \n",
    "                           n_candidates: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Генерирует кандидатов для указанных пользователей.\n",
    "        Кандидаты, которые есть в реальном df, станут позитивными.\n",
    "        Те, которых нет - негативными (hard negatives).\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame с реальными взаимодействиями\n",
    "            users_to_predict: Список ID пользователей\n",
    "            n_candidates: Количество кандидатов на пользователя\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame с кандидатами\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Модель ALS не обучена. Сначала вызовите train()\")\n",
    "        \n",
    "        # Фильтруем пользователей, которых нет в обучении (cold start)\n",
    "        valid_users = [u for u in users_to_predict if u in self.user2idx]\n",
    "        valid_user_idxs = [self.user2idx[u] for u in valid_users]\n",
    "        \n",
    "        if not valid_users:\n",
    "            print(\"Нет валидных пользователей для рекомендаций\")\n",
    "            return pd.DataFrame(columns=[Constants.COL_USER_ID, Constants.COL_BOOK_ID])\n",
    "        \n",
    "        print(f\"Генерация {n_candidates} кандидатов для {len(valid_users)} пользователей...\")\n",
    "        \n",
    "        # Генерация рекомендаций\n",
    "        # filter_already_liked_items=False важно - хотим видеть и то, что уже лайкнул, чтобы разметить правильно\n",
    "        ids, scores = self.model.recommend(\n",
    "            valid_user_idxs,\n",
    "            self.sparse_matrix[valid_user_idxs],\n",
    "            N=n_candidates,\n",
    "            filter_already_liked_items=False\n",
    "        )\n",
    "        \n",
    "        # Собираем кандидатов в DataFrame\n",
    "        candidates_list = []\n",
    "        for i, user_id in enumerate(valid_users):\n",
    "            item_idxs = ids[i]\n",
    "            for item_idx in item_idxs:\n",
    "                book_id = self.idx2item[item_idx]\n",
    "                candidates_list.append([user_id, book_id])\n",
    "        \n",
    "        candidates_df = pd.DataFrame(\n",
    "            candidates_list,\n",
    "            columns=[Constants.COL_USER_ID, Constants.COL_BOOK_ID]\n",
    "        )\n",
    "        \n",
    "        # Добавляем информацию о том, является ли кандидат реальным взаимодействием\n",
    "        real_interactions_set = set(\n",
    "            zip(df[Constants.COL_USER_ID], df[Constants.COL_BOOK_ID])\n",
    "        )\n",
    "        \n",
    "        candidates_df['is_real'] = candidates_df.apply(\n",
    "            lambda row: (row[Constants.COL_USER_ID], \n",
    "                        row[Constants.COL_BOOK_ID]) in real_interactions_set,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        print(f\"Сгенерировано {len(candidates_df)} кандидатов\")\n",
    "        print(f\"  - Реальных взаимодействий: {candidates_df['is_real'].sum()}\")\n",
    "        print(f\"  - Негативных кандидатов: {(~candidates_df['is_real']).sum()}\")\n",
    "        \n",
    "        return candidates_df\n",
    "\n",
    "\n",
    "# ============ ГЕНЕРАЦИЯ ПРИЗНАКОВ ============\n",
    "\n",
    "class TFIDFFeatureExtractor:\n",
    "    \"\"\"Класс для извлечения TF-IDF признаков из текста\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.svd = None\n",
    "        \n",
    "    def fit_transform(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Обучает TF-IDF и SVD на текстах\n",
    "        \n",
    "        Args:\n",
    "            texts: Список текстов\n",
    "            \n",
    "        Returns:\n",
    "            Матрица TF-IDF признаков с пониженной размерностью\n",
    "        \"\"\"\n",
    "        print(\"Обучение TF-IDF и SVD...\")\n",
    "        \n",
    "        # Заполняем пропуски пустыми строками\n",
    "        texts = [str(text) if pd.notna(text) else '' for text in texts]\n",
    "        \n",
    "        # Создаем и обучаем TF-IDF\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=self.config.TFIDF_MAX_FEATURES,\n",
    "            stop_words=None,\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        \n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)\n",
    "        print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "        \n",
    "        # Понижаем размерность с помощью SVD\n",
    "        n_components = min(self.config.TFIDF_SVD_COMPONENTS, tfidf_matrix.shape[1])\n",
    "        self.svd = TruncatedSVD(\n",
    "            n_components=n_components,\n",
    "            random_state=self.config.RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        reduced_features = self.svd.fit_transform(tfidf_matrix)\n",
    "        print(f\"Reduced features shape: {reduced_features.shape}\")\n",
    "        print(f\"Explained variance ratio: {self.svd.explained_variance_ratio_.sum():.3f}\")\n",
    "        \n",
    "        return reduced_features\n",
    "    \n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Преобразует новые тексты в признаки\n",
    "        \n",
    "        Args:\n",
    "            texts: Список текстов\n",
    "            \n",
    "        Returns:\n",
    "            Матрица TF-IDF признаков с пониженной размерностью\n",
    "        \"\"\"\n",
    "        if self.tfidf_vectorizer is None or self.svd is None:\n",
    "            raise ValueError(\"Сначала нужно обучить модель с помощью fit_transform()\")\n",
    "        \n",
    "        texts = [str(text) if pd.notna(text) else '' for text in texts]\n",
    "        tfidf_matrix = self.tfidf_vectorizer.transform(texts)\n",
    "        return self.svd.transform(tfidf_matrix)\n",
    "    \n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Преобразует новые тексты в признаки\n",
    "        \n",
    "        Args:\n",
    "            texts: Список текстов\n",
    "            \n",
    "        Returns:\n",
    "            Матрица TF-IDF признаков с пониженной размерностью\n",
    "        \"\"\"\n",
    "        if self.tfidf_vectorizer is None or self.svd is None:\n",
    "            raise ValueError(\"Сначала нужно обучить модель с помощью fit_transform()\")\n",
    "        \n",
    "        texts = [str(text) if pd.notna(text) else '' for text in texts]\n",
    "        tfidf_matrix = self.tfidf_vectorizer.transform(texts)\n",
    "        return self.svd.transform(tfidf_matrix)\n",
    "\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Класс для генерации признаков\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.constants = Constants()\n",
    "        self.tfidf_extractor = TFIDFFeatureExtractor(config)\n",
    "        self.tfidf_features_columns = None\n",
    "        \n",
    "    def create_genre_features(self, books_meta: pd.DataFrame, \n",
    "                            genres_df: pd.DataFrame, \n",
    "                            book_genres_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Создает признаки на основе жанров книг\n",
    "        \n",
    "        Args:\n",
    "            books_meta: Метаданные книг\n",
    "            genres_df: Справочник жанров\n",
    "            book_genres_df: Связь книг и жанров\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame с признаками жанров\n",
    "        \"\"\"\n",
    "        if genres_df.empty or book_genres_df.empty:\n",
    "            print(\"Нет данных о жанрах, пропускаем создание жанровых признаков\")\n",
    "            return books_meta.copy()\n",
    "        \n",
    "        print(\"Создание жанровых признаков...\")\n",
    "        \n",
    "        # Объединяем жанры с книгами\n",
    "        book_genres = book_genres_df.merge(\n",
    "            genres_df[[self.constants.COL_GENRE_ID, self.constants.COL_GENRE_NAME]], \n",
    "            on=self.constants.COL_GENRE_ID\n",
    "        )\n",
    "        \n",
    "        # Создаем one-hot encoding для топ-N жанров\n",
    "        top_n = 20\n",
    "        top_genres = genres_df.nlargest(top_n, 'books_count')[self.constants.COL_GENRE_ID].tolist()\n",
    "        \n",
    "        # Создаем бинарные признаки для каждого топ-жанра\n",
    "        genre_features = pd.DataFrame(index=books_meta[self.constants.COL_BOOK_ID].unique())\n",
    "        \n",
    "        for genre_id in tqdm(top_genres, desc=\"Creating genre features\"):\n",
    "            books_with_genre = book_genres[\n",
    "                book_genres[self.constants.COL_GENRE_ID] == genre_id\n",
    "            ][self.constants.COL_BOOK_ID].unique()\n",
    "            \n",
    "            genre_features[f'genre_{genre_id}'] = 0\n",
    "            genre_features.loc[genre_features.index.isin(books_with_genre), f'genre_{genre_id}'] = 1\n",
    "        \n",
    "        # Считаем количество жанров у книги\n",
    "        genre_counts = book_genres.groupby(self.constants.COL_BOOK_ID)[self.constants.COL_GENRE_ID].count()\n",
    "        genre_features['genre_count'] = genre_counts\n",
    "        genre_features['genre_count'] = genre_features['genre_count'].fillna(0)\n",
    "        \n",
    "        # Сбрасываем индекс для слияния\n",
    "        genre_features = genre_features.reset_index().rename(\n",
    "            columns={'index': self.constants.COL_BOOK_ID}\n",
    "        )\n",
    "        \n",
    "        # Объединяем с метаданными книг\n",
    "        books_with_genres = books_meta.merge(genre_features, on=self.constants.COL_BOOK_ID, how='left')\n",
    "        \n",
    "        # Заполняем пропуски\n",
    "        genre_columns = [col for col in genre_features.columns if col != self.constants.COL_BOOK_ID]\n",
    "        books_with_genres[genre_columns] = books_with_genres[genre_columns].fillna(0)\n",
    "        \n",
    "        return books_with_genres\n",
    "    \n",
    "    def create_tfidf_features(self, books_meta: pd.DataFrame, \n",
    "                             descriptions_df: pd.DataFrame,\n",
    "                             is_train: bool = True) -> Tuple[pd.DataFrame, List[str]]:\n",
    "        \"\"\"\n",
    "        Создает TF-IDF признаки из описаний книг\n",
    "        \n",
    "        Args:\n",
    "            books_meta: Метаданные книг\n",
    "            descriptions_df: Описания книг\n",
    "            is_train: Если True - обучает TF-IDF, если False - только трансформирует\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[DataFrame с признаками, список имен TF-IDF колонок]\n",
    "        \"\"\"\n",
    "        if descriptions_df.empty:\n",
    "            print(\"Нет данных об описаниях, пропускаем TF-IDF\")\n",
    "            return books_meta.copy(), []\n",
    "        \n",
    "        print(\"Создание TF-IDF признаков...\")\n",
    "        \n",
    "        # Объединяем описания с книгами\n",
    "        books_with_descriptions = books_meta.merge(\n",
    "            descriptions_df, on=self.constants.COL_BOOK_ID, how='left'\n",
    "        )\n",
    "        \n",
    "        # Заполняем пропуски\n",
    "        books_with_descriptions[self.constants.COL_DESCRIPTION] = books_with_descriptions[\n",
    "            self.constants.COL_DESCRIPTION\n",
    "        ].fillna('')\n",
    "        \n",
    "        # Создаем комбинированный текст (название + описание)\n",
    "        books_with_descriptions['combined_text'] = (\n",
    "            books_with_descriptions[self.constants.COL_TITLE].fillna('') + ' ' +\n",
    "            books_with_descriptions[self.constants.COL_DESCRIPTION].fillna('')\n",
    "        )\n",
    "        \n",
    "        # Обучаем или трансформируем TF-IDF\n",
    "        combined_texts = books_with_descriptions['combined_text'].tolist()\n",
    "        \n",
    "        if is_train:\n",
    "            # Для обучения используем fit_transform\n",
    "            tfidf_features_array = self.tfidf_extractor.fit_transform(combined_texts)\n",
    "        else:\n",
    "            # Для валидации/теста используем transform\n",
    "            if self.tfidf_extractor.tfidf_vectorizer is None:\n",
    "                # Если модель не обучена, создаем пустые признаки\n",
    "                print(\"Предупреждение: TF-IDF модель не обучена, создаем пустые признаки\")\n",
    "                n_components = self.config.TFIDF_SVD_COMPONENTS\n",
    "                tfidf_features_array = np.zeros((len(books_with_descriptions), n_components))\n",
    "            else:\n",
    "                tfidf_features_array = self.tfidf_extractor.transform(combined_texts)\n",
    "        \n",
    "        # Создаем имена колонок для TF-IDF признаков\n",
    "        n_features = tfidf_features_array.shape[1]\n",
    "        tfidf_cols = [f'tfidf_{i}' for i in range(n_features)]\n",
    "        \n",
    "        # Преобразуем в DataFrame\n",
    "        tfidf_df = pd.DataFrame(tfidf_features_array, columns=tfidf_cols)\n",
    "        \n",
    "        # Объединяем с исходными данными\n",
    "        result_df = pd.concat([books_with_descriptions, tfidf_df], axis=1)\n",
    "        \n",
    "        # Удаляем временные текстовые колонки\n",
    "        result_df = result_df.drop(['combined_text'], axis=1, errors='ignore')\n",
    "        \n",
    "        return result_df, tfidf_cols\n",
    "    \n",
    "    def create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Создает временные признаки из timestamp\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame с timestamp\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame с добавленными временными признаками\n",
    "        \"\"\"\n",
    "        data = df.copy()\n",
    "        \n",
    "        if self.constants.COL_TIMESTAMP in data.columns:\n",
    "            data['timestamp_dt'] = pd.to_datetime(data[self.constants.COL_TIMESTAMP])\n",
    "            data['year'] = data['timestamp_dt'].dt.year\n",
    "            data['month'] = data['timestamp_dt'].dt.month\n",
    "            data['day'] = data['timestamp_dt'].dt.day\n",
    "            data['dayofweek'] = data['timestamp_dt'].dt.dayofweek\n",
    "            data['hour'] = data['timestamp_dt'].dt.hour\n",
    "            \n",
    "            # Удаляем временную колонку\n",
    "            data = data.drop(columns=['timestamp_dt'])\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def create_user_book_features(self, df: pd.DataFrame, \n",
    "                                 train_interactions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Создает признаки на основе взаимодействий пользователей и книг\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame для добавления признаков\n",
    "            train_interactions: DataFrame с тренировочными взаимодействиями\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame с добавленными признаками\n",
    "        \"\"\"\n",
    "        data = df.copy()\n",
    "        \n",
    "        # --- Статистики пользователей ---\n",
    "        print(\"Создание пользовательских статистик...\")\n",
    "        user_stats = train_interactions.groupby(self.constants.COL_USER_ID).agg({\n",
    "            self.constants.COL_RATING: ['mean', 'std', 'count', 'max', 'min'],\n",
    "            self.constants.COL_HAS_READ: ['sum', 'mean', 'std'],\n",
    "            self.constants.COL_TIMESTAMP: ['max', 'min', 'nunique'] if self.constants.COL_TIMESTAMP in train_interactions.columns else []\n",
    "        })\n",
    "        \n",
    "        # Выравниваем мультииндекс\n",
    "        user_stats.columns = ['_'.join(col).strip() for col in user_stats.columns.values]\n",
    "        user_stats = user_stats.reset_index()\n",
    "        \n",
    "        # Переименовываем колонки\n",
    "        rename_dict = {\n",
    "            'rating_mean': 'user_avg_rating',\n",
    "            'rating_std': 'user_rating_std',\n",
    "            'rating_count': 'user_activity_count',\n",
    "            'rating_max': 'user_max_rating',\n",
    "            'rating_min': 'user_min_rating',\n",
    "            'has_read_sum': 'user_read_count',\n",
    "            'has_read_mean': 'user_read_ratio',\n",
    "            'has_read_std': 'user_read_std',\n",
    "            'timestamp_max': 'user_last_activity',\n",
    "            'timestamp_min': 'user_first_activity',\n",
    "            'timestamp_nunique': 'user_active_days'\n",
    "        }\n",
    "        \n",
    "        user_stats = user_stats.rename(columns={k: v for k, v in rename_dict.items() if k in user_stats.columns})\n",
    "        \n",
    "        # --- Статистики книг ---\n",
    "        print(\"Создание книжных статистик...\")\n",
    "        book_stats = train_interactions.groupby(self.constants.COL_BOOK_ID).agg({\n",
    "            self.constants.COL_RATING: ['mean', 'std', 'count', 'max', 'min'],\n",
    "            self.constants.COL_HAS_READ: ['sum', 'mean', 'std'],\n",
    "            self.constants.COL_USER_ID: 'nunique'\n",
    "        })\n",
    "        \n",
    "        book_stats.columns = ['_'.join(col).strip() for col in book_stats.columns.values]\n",
    "        book_stats = book_stats.reset_index()\n",
    "        \n",
    "        rename_dict = {\n",
    "            'rating_mean': 'book_avg_rating',\n",
    "            'rating_std': 'book_rating_std',\n",
    "            'rating_count': 'book_popularity',\n",
    "            'rating_max': 'book_max_rating',\n",
    "            'rating_min': 'book_min_rating',\n",
    "            'has_read_sum': 'book_read_count',\n",
    "            'has_read_mean': 'book_read_ratio',\n",
    "            'has_read_std': 'book_read_std',\n",
    "            'user_id_nunique': 'book_unique_users'\n",
    "        }\n",
    "        \n",
    "        book_stats = book_stats.rename(columns={k: v for k, v in rename_dict.items() if k in book_stats.columns})\n",
    "        \n",
    "        # Объединяем статистики\n",
    "        data = data.merge(user_stats, on=self.constants.COL_USER_ID, how='left')\n",
    "        data = data.merge(book_stats, on=self.constants.COL_BOOK_ID, how='left')\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def create_advanced_features(self, data: pd.DataFrame, \n",
    "                                train_interactions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Создает продвинутые признаки\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame с базовыми признаками\n",
    "            train_interactions: Тренировочные взаимодействия\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame с добавленными продвинутыми признаками\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        \n",
    "        # --- Взаимодействия между пользователями и авторами ---\n",
    "        print(\"Создание признаков пользователь-автор...\")\n",
    "        \n",
    "        # Количество взаимодействий пользователя с автором\n",
    "        user_author_stats = train_interactions.merge(\n",
    "            df[[self.constants.COL_BOOK_ID, self.constants.COL_AUTHOR_ID]].drop_duplicates(),\n",
    "            on=self.constants.COL_BOOK_ID,\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        if not user_author_stats.empty:\n",
    "            user_author_agg = user_author_stats.groupby(\n",
    "                [self.constants.COL_USER_ID, self.constants.COL_AUTHOR_ID]\n",
    "            ).agg({\n",
    "                self.constants.COL_RATING: ['mean', 'count'],\n",
    "                self.constants.COL_HAS_READ: 'sum'\n",
    "            })\n",
    "            \n",
    "            user_author_agg.columns = ['_'.join(col).strip() for col in user_author_agg.columns.values]\n",
    "            user_author_agg = user_author_agg.reset_index()\n",
    "            user_author_agg = user_author_agg.rename(columns={\n",
    "                'rating_mean': 'user_author_avg_rating',\n",
    "                'rating_count': 'user_author_interactions',\n",
    "                'has_read_sum': 'user_author_read_count'\n",
    "            })\n",
    "            \n",
    "            df = df.merge(\n",
    "                user_author_agg, \n",
    "                on=[self.constants.COL_USER_ID, self.constants.COL_AUTHOR_ID], \n",
    "                how='left'\n",
    "            )\n",
    "        \n",
    "        # --- Разница между средним рейтингом пользователя и книги ---\n",
    "        if 'user_avg_rating' in df.columns and 'book_avg_rating' in df.columns:\n",
    "            df['rating_diff'] = df['user_avg_rating'] - df['book_avg_rating']\n",
    "            df['rating_diff_abs'] = df['rating_diff'].abs()\n",
    "        \n",
    "        # --- Время с последней активности пользователя (если есть timestamp) ---\n",
    "        if self.constants.COL_TIMESTAMP in train_interactions.columns:\n",
    "            latest_timestamp = train_interactions[self.constants.COL_TIMESTAMP].max()\n",
    "            train_interactions['days_since_last'] = (\n",
    "                latest_timestamp - train_interactions[self.constants.COL_TIMESTAMP]\n",
    "            ).dt.days\n",
    "            \n",
    "            user_recency = train_interactions.groupby(self.constants.COL_USER_ID)['days_since_last'].min()\n",
    "            df = df.merge(user_recency.rename('user_days_since_last'), \n",
    "                         on=self.constants.COL_USER_ID, how='left')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def generate_features(self, df: pd.DataFrame, users_meta: pd.DataFrame,\n",
    "                         books_meta: pd.DataFrame, train_interactions: pd.DataFrame,\n",
    "                         val_interactions: pd.DataFrame = None,  # Добавляем val_interactions\n",
    "                         genres_df: pd.DataFrame = None, \n",
    "                         book_genres_df: pd.DataFrame = None,\n",
    "                         descriptions_df: pd.DataFrame = None,\n",
    "                         is_train: bool = True) -> pd.DataFrame:  # Флаг - тренировочные ли данные\n",
    "        \"\"\"\n",
    "        Основной метод для генерации всех признаков\n",
    "        \n",
    "        Args:\n",
    "            df: Исходный DataFrame\n",
    "            users_meta: Метаданные пользователей\n",
    "            books_meta: Метаданные книг\n",
    "            train_interactions: Тренировочные взаимодействия (ТОЛЬКО для train)\n",
    "            val_interactions: Валидационные взаимодействия (для validation)\n",
    "            genres_df: Справочник жанров\n",
    "            book_genres_df: Связь книг и жанров\n",
    "            descriptions_df: Описания книг\n",
    "            is_train: Флаг - генерируем признаки для train или val/test\n",
    "        \"\"\"\n",
    "        print(\"Генерация признаков...\")\n",
    "        \n",
    "        # Копируем данные\n",
    "        data = df.copy()\n",
    "        \n",
    "        # 1. Добавляем метаданные пользователей\n",
    "        print(\"1. Добавление метаданных пользователей...\")\n",
    "        data = data.merge(users_meta, on=self.constants.COL_USER_ID, how='left')\n",
    "        \n",
    "        # 2. Обрабатываем метаданные книг с TF-IDF и жанрами\n",
    "        print(\"2. Обработка метаданных книг...\")\n",
    "        processed_books = books_meta.copy()\n",
    "        \n",
    "        # Добавляем жанровые признаки если есть данные\n",
    "        if genres_df is not None and book_genres_df is not None:\n",
    "            processed_books = self.create_genre_features(processed_books, genres_df, book_genres_df)\n",
    "        \n",
    "        # Добавляем TF-IDF признаки если есть описания\n",
    "        if descriptions_df is not None and not descriptions_df.empty:\n",
    "            processed_books, tfidf_cols = self.create_tfidf_features(processed_books, descriptions_df)\n",
    "        else:\n",
    "            tfidf_cols = []\n",
    "        \n",
    "        # Объединяем обработанные метаданные книг\n",
    "        data = data.merge(processed_books, on=self.constants.COL_BOOK_ID, how='left')\n",
    "        \n",
    "        # 3. КРИТИЧЕСКОЕ ИЗМЕНЕНИЕ: Разные данные для вычисления статистик\n",
    "        print(\"3. Создание признаков взаимодействий...\")\n",
    "        \n",
    "        if is_train:\n",
    "            # Для тренировочных данных используем только train_interactions\n",
    "            stats_data = train_interactions\n",
    "        else:\n",
    "            # Для валидационных/тестовых данных используем только train_interactions\n",
    "            # НЕ используем val_interactions для вычисления статистик!\n",
    "            stats_data = train_interactions\n",
    "        \n",
    "        data = self.create_user_book_features(data, stats_data)\n",
    "        \n",
    "        # 4. Создаем продвинутые признаки\n",
    "        print(\"4. Создание продвинутых признаков...\")\n",
    "        data = self.create_advanced_features(data, stats_data)  # Используем stats_data здесь тоже\n",
    "        \n",
    "        # 5. Создаем временные признаки если есть timestamp\n",
    "        if self.constants.COL_TIMESTAMP in data.columns:\n",
    "            print(\"5. Создание временных признаков...\")\n",
    "            data = self.create_temporal_features(data)\n",
    "        \n",
    "        \n",
    "        # 6. Обработка пропусков\n",
    "        print(\"6. Обработка пропусков...\")\n",
    "        \n",
    "        # Заполняем числовые признаки\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in [\n",
    "            self.constants.COL_USER_ID, self.constants.COL_BOOK_ID, self.constants.COL_TARGET\n",
    "        ]]\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in data.columns:\n",
    "                if 'user_' in col:\n",
    "                    # Для пользовательских признаков заполняем глобальным средним или 0\n",
    "                    if col.endswith('_ratio') or col.endswith('_mean'):\n",
    "                        fill_value = train_interactions[self.constants.COL_RATING].mean() if 'rating' in col else 0\n",
    "                    else:\n",
    "                        fill_value = 0\n",
    "                elif 'book_' in col:\n",
    "                    # Для книжных признаков\n",
    "                    if col.endswith('_rating'):\n",
    "                        fill_value = train_interactions[self.constants.COL_RATING].mean()\n",
    "                    else:\n",
    "                        fill_value = 0\n",
    "                else:\n",
    "                    fill_value = data[col].median() if data[col].notna().any() else 0\n",
    "                \n",
    "                data[col] = data[col].fillna(fill_value)\n",
    "        \n",
    "        # 7. Обрабатываем категориальные признаки\n",
    "        print(\"7. Обработка категориальных признаков...\")\n",
    "        for cat_feature in self.config.CATEGORICAL_FEATURES:\n",
    "            if cat_feature in data.columns:\n",
    "                # Для категориальных признаков заполняем -1 и преобразуем в int\n",
    "                data[cat_feature] = data[cat_feature].fillna(-1).astype(int)\n",
    "        \n",
    "        # 8. Удаляем текстовые колонки (CatBoost не может обработать их как числовые)\n",
    "        print(\"8. Удаление текстовых колонок...\")\n",
    "        text_cols_to_drop = [col for col in self.config.TEXT_FEATURES_TO_DROP if col in data.columns]\n",
    "        if text_cols_to_drop:\n",
    "            print(f\"Удаляемые текстовые колонки: {text_cols_to_drop}\")\n",
    "            data = data.drop(columns=text_cols_to_drop)\n",
    "        \n",
    "        # Также удаляем любые другие нечисловые колонки\n",
    "        non_numeric_cols = data.select_dtypes(exclude=[np.number]).columns\n",
    "        non_numeric_cols = [col for col in non_numeric_cols if col not in [\n",
    "            self.constants.COL_USER_ID, self.constants.COL_BOOK_ID, self.constants.COL_TARGET\n",
    "        ]]\n",
    "        \n",
    "        if non_numeric_cols:\n",
    "            print(f\"Удаляемые нечисловые колонки: {non_numeric_cols}\")\n",
    "            data = data.drop(columns=non_numeric_cols)\n",
    "        \n",
    "        print(f\"Признаки сгенерированы. Итоговая форма: {data.shape}\")\n",
    "        print(f\"Количество признаков: {len(data.columns) - 3}\")  # -3 для user_id, book_id, target\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def process_features(self, df: pd.DataFrame,\n",
    "                        users_meta: pd.DataFrame,\n",
    "                        books_meta: pd.DataFrame,\n",
    "                        train_interactions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Обертка для generate_features (для совместимости)\n",
    "        \"\"\"\n",
    "        return self.generate_features(df, users_meta, books_meta, train_interactions)\n",
    "\n",
    "\n",
    "# ============ ИСПРАВЛЕННЫЙ SubmissionGenerator ============\n",
    "\n",
    "class SubmissionGenerator:\n",
    "    \"\"\"Класс для формирования финального сабмита\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.constants = Constants()\n",
    "    \n",
    "    def prepare_submission_format(self, candidates_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Преобразует candidates.csv в формат для предсказания\n",
    "        \n",
    "        Args:\n",
    "            candidates_df: DataFrame с кандидатами в формате user_id, book_id_list\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame в формате user_id, book_id\n",
    "        \"\"\"\n",
    "        print(\"Подготовка данных для предсказания...\")\n",
    "        \n",
    "        # Проверяем формат candidates_df\n",
    "        if 'book_id_list' in candidates_df.columns:\n",
    "            # Преобразуем строку с book_id_list в список book_id\n",
    "            candidates_long = []\n",
    "            \n",
    "            for _, row in tqdm(candidates_df.iterrows(), total=len(candidates_df), desc=\"Processing candidates\"):\n",
    "                user_id = row['user_id']\n",
    "                book_id_str = str(row['book_id_list'])\n",
    "                \n",
    "                # Разделяем строку по запятым и удаляем дубликаты\n",
    "                book_ids = []\n",
    "                seen = set()\n",
    "                for bid in book_id_str.split(','):\n",
    "                    bid_clean = bid.strip()\n",
    "                    if bid_clean and bid_clean not in seen:\n",
    "                        book_ids.append(int(bid_clean))\n",
    "                        seen.add(bid_clean)\n",
    "                \n",
    "                for book_id in book_ids:\n",
    "                    candidates_long.append([user_id, book_id])\n",
    "            \n",
    "            candidates_long_df = pd.DataFrame(candidates_long, columns=['user_id', 'book_id'])\n",
    "        else:\n",
    "            # Уже в длинном формате\n",
    "            candidates_long_df = candidates_df.copy()\n",
    "        \n",
    "        # Удаляем дубликаты пар (user_id, book_id)\n",
    "        candidates_long_df = candidates_long_df.drop_duplicates(subset=['user_id', 'book_id'])\n",
    "        \n",
    "        print(f\"Преобразовано в длинный формат: {len(candidates_long_df)} уникальных пар (user_id, book_id)\")\n",
    "        return candidates_long_df\n",
    "    \n",
    "    def generate_submission(self, pipeline: 'RankingPipeline', candidates_df: pd.DataFrame,\n",
    "                          users_df: pd.DataFrame, books_df: pd.DataFrame, \n",
    "                          train_df: pd.DataFrame, \n",
    "                          book_descriptions: pd.DataFrame = None,\n",
    "                          genres_df: pd.DataFrame = None,\n",
    "                          book_genres_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Генерирует финальный сабмит\n",
    "        \n",
    "        Args:\n",
    "            pipeline: Обученный пайплайн\n",
    "            candidates_df: DataFrame с кандидатами\n",
    "            users_df: Метаданные пользователей\n",
    "            books_df: Метаданные книг\n",
    "            train_df: Обучающие данные для вычисления статистик\n",
    "            book_descriptions: Описания книг\n",
    "            genres_df: Справочник жанров\n",
    "            book_genres_df: Связь книг и жанров\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame с сабмитом в правильном формате\n",
    "        \"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ФОРМИРОВАНИЕ САБМИТА\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Подготавливаем кандидатов в длинном формате\n",
    "        print(\"\\n1. Подготовка кандидатов...\")\n",
    "        candidates_long = self.prepare_submission_format(candidates_df)\n",
    "        \n",
    "        # 2. Генерируем признаки для кандидатов\n",
    "        print(\"\\n2. Генерация признаков для кандидатов...\")\n",
    "        \n",
    "        # Создаем фиктивный target для кандидатов\n",
    "        candidates_long['target'] = 0\n",
    "        \n",
    "        # Генерируем признаки\n",
    "        X_candidates = pipeline.feature_engineer.generate_features(\n",
    "            candidates_long,\n",
    "            users_df,\n",
    "            books_df,\n",
    "            train_df,  # train_interactions\n",
    "            None,      # val_interactions - передаем None, так как для сабмита нет валидационных данных\n",
    "            genres_df,\n",
    "            book_genres_df,\n",
    "            book_descriptions,\n",
    "            is_train=False  # Важно! Для сабмита это не тренировочные данные\n",
    "        )\n",
    "        \n",
    "        # 3. Делаем предсказания\n",
    "        print(\"\\n3. Предсказание релевантности...\")\n",
    "        \n",
    "        # Подготавливаем данные для предсказания\n",
    "        features_to_drop = ['user_id', 'book_id', 'target']\n",
    "        X_pred = X_candidates.drop(features_to_drop, axis=1)\n",
    "        \n",
    "        # Делаем предсказания\n",
    "        predictions = pipeline.model.predict(X_pred)\n",
    "        \n",
    "        # Добавляем предсказания\n",
    "        X_candidates['prediction'] = predictions\n",
    "        \n",
    "        # 4. Формируем сабмит в требуемом формате\n",
    "        print(\"\\n4. Формирование сабмита...\")\n",
    "        \n",
    "        # Сортируем по user_id и prediction (по убыванию)\n",
    "        X_candidates = X_candidates.sort_values(['user_id', 'prediction'], ascending=[True, False])\n",
    "        \n",
    "        # Группируем по user_id и собираем топ-20 book_id\n",
    "        submission_list = []\n",
    "        \n",
    "        for user_id, group in tqdm(X_candidates.groupby('user_id'), desc=\"Forming submission\"):\n",
    "            # Удаляем дубликаты book_id в рамках одного пользователя\n",
    "            group_unique = group.drop_duplicates(subset='book_id')\n",
    "            \n",
    "            # Берем топ-20 или меньше, если кандидатов меньше\n",
    "            top_k = min(20, len(group_unique))\n",
    "            top_books = group_unique.head(top_k)['book_id'].tolist()\n",
    "            \n",
    "            # Проверяем на дубликаты\n",
    "            if len(top_books) != len(set(top_books)):\n",
    "                # Удаляем дубликаты, сохраняя порядок\n",
    "                seen = set()\n",
    "                top_books_unique = []\n",
    "                for book_id in top_books:\n",
    "                    if book_id not in seen:\n",
    "                        seen.add(book_id)\n",
    "                        top_books_unique.append(book_id)\n",
    "                top_books = top_books_unique[:20]  # Берем снова топ-20 после удаления дубликатов\n",
    "            \n",
    "            # Преобразуем список в строку, разделенную запятыми\n",
    "            book_id_list = ','.join(map(str, top_books))\n",
    "            submission_list.append([user_id, book_id_list])\n",
    "        \n",
    "        # Создаем DataFrame сабмита\n",
    "        submission_df = pd.DataFrame(submission_list, columns=['user_id', 'book_id_list'])\n",
    "        \n",
    "        # 5. Проверяем сабмит на соответствие требованиям\n",
    "        print(\"\\n5. Проверка сабмита...\")\n",
    "        submission_df = self._validate_submission(submission_df)\n",
    "        \n",
    "        print(f\"\\nСабмит сформирован для {len(submission_df)} пользователей\")\n",
    "        print(f\"Среднее количество книг на пользователя: {submission_df['book_id_list'].apply(lambda x: len(x.split(',')) if x else 0).mean():.1f}\")\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    def _validate_submission(self, submission_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Проверяет сабмит на соответствие требованиям\n",
    "        \n",
    "        Args:\n",
    "            submission_df: DataFrame с сабмитом\n",
    "            \n",
    "        Returns:\n",
    "            Проверенный DataFrame\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for idx, row in submission_df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            book_id_list = row['book_id_list']\n",
    "            \n",
    "            # Проверяем, что book_id_list не пустой\n",
    "            if not book_id_list:\n",
    "                errors.append({\n",
    "                    'row': idx,\n",
    "                    'user_id': user_id,\n",
    "                    'error': 'Empty book_id_list'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Разбираем список book_id\n",
    "            book_ids = [bid.strip() for bid in book_id_list.split(',')]\n",
    "            \n",
    "            # Проверяем, что все book_id - числа\n",
    "            for bid in book_ids:\n",
    "                if not bid.isdigit():\n",
    "                    errors.append({\n",
    "                        'row': idx,\n",
    "                        'user_id': user_id,\n",
    "                        'error': f'Non-numeric book_id: {bid}'\n",
    "                    })\n",
    "            \n",
    "            # Проверяем на дубликаты\n",
    "            if len(book_ids) != len(set(book_ids)):\n",
    "                duplicates = set([bid for bid in book_ids if book_ids.count(bid) > 1])\n",
    "                errors.append({\n",
    "                    'row': idx,\n",
    "                    'user_id': user_id,\n",
    "                    'error': f'Duplicate book_ids in list: {duplicates}'\n",
    "                })\n",
    "            \n",
    "            # Проверяем количество книг (не более 20)\n",
    "            if len(book_ids) > 20:\n",
    "                errors.append({\n",
    "                    'row': idx,\n",
    "                    'user_id': user_id,\n",
    "                    'error': f'More than 20 books: {len(book_ids)}'\n",
    "                })\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"\\nНайдено {len(errors)} ошибок в сабмите:\")\n",
    "            for error in errors[:5]:  # Показываем только первые 5 ошибок\n",
    "                print(f\"  Строка {error['row']}, user_id={error['user_id']}: {error['error']}\")\n",
    "            if len(errors) > 5:\n",
    "                print(f\"  ... и еще {len(errors) - 5} ошибок\")\n",
    "            \n",
    "            # Автоматически исправляем ошибки\n",
    "            print(\"\\nАвтоматическое исправление ошибок...\")\n",
    "            submission_df = self._fix_submission_errors(submission_df, errors)\n",
    "        \n",
    "        else:\n",
    "            print(\"✓ Сабмит прошел все проверки!\")\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    def _fix_submission_errors(self, submission_df: pd.DataFrame, errors: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Автоматически исправляет ошибки в сабмите\n",
    "        \n",
    "        Args:\n",
    "            submission_df: DataFrame с сабмитом\n",
    "            errors: Список ошибок\n",
    "            \n",
    "        Returns:\n",
    "            Исправленный DataFrame\n",
    "        \"\"\"\n",
    "        submission_df_fixed = submission_df.copy()\n",
    "        \n",
    "        for error in errors:\n",
    "            idx = error['row']\n",
    "            user_id = error['user_id']\n",
    "            \n",
    "            # Исправляем дубликаты\n",
    "            if 'Duplicate' in error['error']:\n",
    "                book_id_list = submission_df_fixed.at[idx, 'book_id_list']\n",
    "                if pd.isna(book_id_list):\n",
    "                    continue\n",
    "                \n",
    "                # Удаляем дубликаты, сохраняя порядок\n",
    "                book_ids = [bid.strip() for bid in book_id_list.split(',')]\n",
    "                seen = set()\n",
    "                unique_book_ids = []\n",
    "                \n",
    "                for bid in book_ids:\n",
    "                    if bid not in seen:\n",
    "                        seen.add(bid)\n",
    "                        unique_book_ids.append(bid)\n",
    "                \n",
    "                # Ограничиваем 20 книгами\n",
    "                unique_book_ids = unique_book_ids[:20]\n",
    "                \n",
    "                # Обновляем строку\n",
    "                submission_df_fixed.at[idx, 'book_id_list'] = ','.join(unique_book_ids)\n",
    "        \n",
    "        print(\"✓ Автоматическое исправление завершено\")\n",
    "        \n",
    "        # Проверяем снова\n",
    "        print(\"\\nПовторная проверка после исправления...\")\n",
    "        submission_df_fixed = self._validate_submission(submission_df_fixed)\n",
    "        \n",
    "        return submission_df_fixed\n",
    "    \n",
    "    def save_submission(self, submission_df: pd.DataFrame, filename: str = \"submission.csv\"):\n",
    "        \"\"\"\n",
    "        Сохраняет сабмит в файл\n",
    "        \n",
    "        Args:\n",
    "            submission_df: DataFrame с сабмитом\n",
    "            filename: Имя файла для сохранения\n",
    "        \"\"\"\n",
    "        submission_path = self.config.RESULTS_DIR / filename\n",
    "        \n",
    "        # Сохраняем без индекса\n",
    "        submission_df.to_csv(submission_path, index=False)\n",
    "        \n",
    "        print(f\"\\nСабмит сохранен: {submission_path}\")\n",
    "        print(f\"Размер файла: {submission_path.stat().st_size / 1024:.1f} KB\")\n",
    "        \n",
    "        # Создаем пример для проверки\n",
    "        sample_path = self.config.RESULTS_DIR / \"submission_sample.txt\"\n",
    "        with open(sample_path, 'w') as f:\n",
    "            f.write(\"Пример формата сабмита:\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            f.write(submission_df.head(10).to_string())\n",
    "            f.write(\"\\n\\nСтатистика:\\n\")\n",
    "            f.write(f\"Количество пользователей: {len(submission_df)}\\n\")\n",
    "            \n",
    "            book_counts = submission_df['book_id_list'].apply(lambda x: len(x.split(',')) if x else 0)\n",
    "            f.write(f\"Среднее количество книг на пользователя: {book_counts.mean():.1f}\\n\")\n",
    "            f.write(f\"Мин. количество книг: {book_counts.min()}\\n\")\n",
    "            f.write(f\"Макс. количество книг: {book_counts.max()}\\n\")\n",
    "            \n",
    "            # Проверка уникальности user_id\n",
    "            if submission_df['user_id'].nunique() == len(submission_df):\n",
    "                f.write(\"✓ Все user_id уникальны\\n\")\n",
    "            else:\n",
    "                f.write(f\"✗ Есть дубликаты user_id: {len(submission_df) - submission_df['user_id'].nunique()}\\n\")\n",
    "        \n",
    "        print(f\"Пример сохранен в: {sample_path}\")\n",
    "\n",
    "\n",
    "# ============ ОБНОВЛЕННЫЙ RankingPipeline ============\n",
    "\n",
    "class RankingPipeline:\n",
    "    \"\"\"Основной пайплайн для ранжирования\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.constants = Constants()\n",
    "        self.data_loader = DataLoader(config)\n",
    "        self.feature_engineer = FeatureEngineer(config)\n",
    "        self.submission_generator = SubmissionGenerator(config)\n",
    "        self.model = None\n",
    "        self.data = None\n",
    "    \n",
    "    def run(self, train_mode: bool = True):\n",
    "        \"\"\"Запуск полного пайплайна\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ЗАПУСК ПАЙПЛАЙНА РАНЖИРОВАНИЯ\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Загрузка данных\n",
    "        print(\"\\n1. Загрузка данных...\")\n",
    "        self.data = self.data_loader.load_all_data()\n",
    "        \n",
    "        train_df = self.data['train']\n",
    "        candidates_df = self.data['candidates']\n",
    "        users_df = self.data['users']\n",
    "        books_df = self.data['books']\n",
    "        book_descriptions = self.data.get('book_descriptions', pd.DataFrame())\n",
    "        genres_df = self.data.get('genres', pd.DataFrame())\n",
    "        book_genres_df = self.data.get('book_genres', pd.DataFrame())\n",
    "        \n",
    "        # Подготовка целевой переменной\n",
    "        train_df = self.data_loader.prepare_target(train_df)\n",
    "        \n",
    "        if train_mode:\n",
    "            # 2. Разделение на train/val С УЧЕТОМ ВРЕМЕНИ\n",
    "            print(\"\\n2. Разделение данных с учетом времени...\")\n",
    "            \n",
    "            # Сортируем по времени\n",
    "            train_df = train_df.sort_values(self.constants.COL_TIMESTAMP)\n",
    "            \n",
    "            # Разделяем по времени (например, 90% времени - train, 10% - val)\n",
    "            split_time = train_df[self.constants.COL_TIMESTAMP].quantile(0.9)\n",
    "            \n",
    "            train_interactions = train_df[train_df[self.constants.COL_TIMESTAMP] < split_time].copy()\n",
    "            val_interactions = train_df[train_df[self.constants.COL_TIMESTAMP] >= split_time].copy()\n",
    "            \n",
    "            # Берем только пользователей, которые есть в train\n",
    "            val_users_in_train = set(train_interactions[self.constants.COL_USER_ID].unique())\n",
    "            val_interactions = val_interactions[\n",
    "                val_interactions[self.constants.COL_USER_ID].isin(val_users_in_train)\n",
    "            ]\n",
    "            \n",
    "            print(f\"Train interactions: {len(train_interactions)} (до {split_time})\")\n",
    "            print(f\"Val interactions: {len(val_interactions)} (после {split_time})\")\n",
    "            \n",
    "            # 3. Генерация кандидатов с помощью ALS\n",
    "            print(\"\\n3. Генерация кандидатов с помощью ALS...\")\n",
    "            \n",
    "            # Обучаем ALS на тренировочных взаимодействиях (ДО split_time)\n",
    "            als_recommender = ALSRecommender(self.config)\n",
    "            als_recommender.train(train_interactions)\n",
    "            \n",
    "            # Генерируем кандидатов для пользователей в валидации\n",
    "            val_users = val_interactions[self.constants.COL_USER_ID].unique()\n",
    "            n_candidates = self.config.ALS_TOP_K_CANDIDATES\n",
    "            \n",
    "            generated_candidates_val = als_recommender.generate_candidates(\n",
    "                train_interactions,  # Только train для генерации кандидатов\n",
    "                val_users,\n",
    "                n_candidates=n_candidates\n",
    "            )\n",
    "            \n",
    "            # Для тренировочных пользователей тоже генерируем кандидатов\n",
    "            train_users = train_interactions[self.constants.COL_USER_ID].unique()\n",
    "            generated_candidates_train = als_recommender.generate_candidates(\n",
    "                train_interactions,\n",
    "                train_users,\n",
    "                n_candidates=n_candidates\n",
    "            )\n",
    "            \n",
    "            # Объединяем кандидатов\n",
    "            generated_candidates = pd.concat([\n",
    "                generated_candidates_train,\n",
    "                generated_candidates_val\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            # 4. Размечаем кандидатов\n",
    "            print(\"\\n4. Разметка кандидатов...\")\n",
    "            \n",
    "            # Создаем множества реальных взаимодействий для быстрой проверки\n",
    "            real_interactions_train_set = set(\n",
    "                zip(train_interactions[self.constants.COL_USER_ID], \n",
    "                    train_interactions[self.constants.COL_BOOK_ID])\n",
    "            )\n",
    "            \n",
    "            real_interactions_val_set = set(\n",
    "                zip(val_interactions[self.constants.COL_USER_ID], \n",
    "                    val_interactions[self.constants.COL_BOOK_ID])\n",
    "            )\n",
    "            \n",
    "            def is_real_interaction(row):\n",
    "                user_id = row[self.constants.COL_USER_ID]\n",
    "                book_id = row[self.constants.COL_BOOK_ID]\n",
    "                \n",
    "                if user_id in train_users_set:\n",
    "                    return (user_id, book_id) in real_interactions_train_set\n",
    "                else:\n",
    "                    return (user_id, book_id) in real_interactions_val_set\n",
    "            \n",
    "            train_users_set = set(train_users)\n",
    "            generated_candidates['is_real'] = generated_candidates.apply(is_real_interaction, axis=1)\n",
    "            \n",
    "            # 5. Создаем финальный датасет с таргетами\n",
    "            print(\"\\n5. Создание финального датасета...\")\n",
    "            \n",
    "            # Позитивные примеры из train\n",
    "            train_positives = train_interactions[[\n",
    "                self.constants.COL_USER_ID, \n",
    "                self.constants.COL_BOOK_ID, \n",
    "                self.constants.COL_TARGET\n",
    "            ]].copy()\n",
    "            \n",
    "            # Позитивные примеры из val\n",
    "            val_positives = val_interactions[[\n",
    "                self.constants.COL_USER_ID, \n",
    "                self.constants.COL_BOOK_ID, \n",
    "                self.constants.COL_TARGET\n",
    "            ]].copy()\n",
    "            \n",
    "            # Негативные примеры (кандидаты, которых нет в реальных взаимодействиях)\n",
    "            negative_candidates = generated_candidates[~generated_candidates['is_real']].copy()\n",
    "            negative_candidates[self.constants.COL_TARGET] = self.constants.TARGET_NEGATIVE\n",
    "            \n",
    "            # Объединяем\n",
    "            full_dataset = pd.concat([\n",
    "                train_positives,\n",
    "                val_positives,\n",
    "                negative_candidates[[self.constants.COL_USER_ID, \n",
    "                                     self.constants.COL_BOOK_ID, \n",
    "                                     self.constants.COL_TARGET]]\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            print(f\"\\nИтоговый размер датасета: {len(full_dataset)}\")\n",
    "            print(f\"  - Позитивных (train): {len(train_positives)}\")\n",
    "            print(f\"  - Позитивных (val): {len(val_positives)}\")\n",
    "            print(f\"  - Негативных: {len(negative_candidates)}\")\n",
    "            \n",
    "            # 6. Генерация признаков для train и val раздельно\n",
    "            print(\"\\n6. Генерация признаков...\")\n",
    "            \n",
    "            # Разделяем данные на train и val части\n",
    "            train_mask = full_dataset[self.constants.COL_USER_ID].isin(train_users_set)\n",
    "            val_mask = ~train_mask\n",
    "            \n",
    "            # Генерируем признаки для train\n",
    "            print(\"  Генерация признаков для train...\")\n",
    "            X_train = self.feature_engineer.generate_features(\n",
    "                full_dataset[train_mask],\n",
    "                users_df,\n",
    "                books_df,\n",
    "                train_interactions,  # Только train для статистик\n",
    "                val_interactions,    # Передаем, но не используем в is_train=True\n",
    "                genres_df,\n",
    "                book_genres_df,\n",
    "                book_descriptions,\n",
    "                is_train=True\n",
    "            )\n",
    "            \n",
    "            # Генерируем признаки для val\n",
    "            print(\"  Генерация признаков для val...\")\n",
    "            X_val = self.feature_engineer.generate_features(\n",
    "                full_dataset[val_mask],\n",
    "                users_df,\n",
    "                books_df,\n",
    "                train_interactions,  # Только train для статистик!\n",
    "                val_interactions,\n",
    "                genres_df,\n",
    "                book_genres_df,\n",
    "                book_descriptions,\n",
    "                is_train=False  # Важно! Для val не используем val_interactions в статистиках\n",
    "            )\n",
    "            \n",
    "            # Объединяем\n",
    "            X_full = pd.concat([X_train, X_val], ignore_index=True)\n",
    "            \n",
    "            # Определяем списки признаков\n",
    "            categorical_features = [c for c in self.config.CATEGORICAL_FEATURES if c in X_full.columns]\n",
    "            \n",
    "            print(f\"\\nКатегориальные признаки ({len(categorical_features)}): {categorical_features[:10]}...\")\n",
    "            \n",
    "            # 7. Подготовка данных для CatBoostRanker\n",
    "            print(\"\\n7. Подготовка данных для обучения...\")\n",
    "            \n",
    "            # Сортируем по user_id (требование CatBoostRanker)\n",
    "            X_full = X_full.sort_values(by=self.constants.COL_USER_ID)\n",
    "            \n",
    "            # Разделяем на признаки и целевую переменную\n",
    "            features_to_drop = [self.constants.COL_USER_ID, \n",
    "                               self.constants.COL_BOOK_ID, \n",
    "                               self.constants.COL_TARGET]\n",
    "            \n",
    "            X = X_full.drop(features_to_drop, axis=1)\n",
    "            y = X_full[self.constants.COL_TARGET]\n",
    "            group_id = X_full[self.constants.COL_USER_ID]\n",
    "            \n",
    "            # Разделяем на train и val по пользователям\n",
    "            train_mask_full = X_full[self.constants.COL_USER_ID].isin(train_users)\n",
    "            val_mask_full = X_full[self.constants.COL_USER_ID].isin(val_users)\n",
    "            \n",
    "            # Создаем Pool объекты\n",
    "            train_pool = Pool(\n",
    "                data=X[train_mask_full],\n",
    "                label=y[train_mask_full],\n",
    "                group_id=group_id[train_mask_full],\n",
    "                cat_features=categorical_features\n",
    "            )\n",
    "            \n",
    "            val_pool = Pool(\n",
    "                data=X[val_mask_full],\n",
    "                label=y[val_mask_full],\n",
    "                group_id=group_id[val_mask_full],\n",
    "                cat_features=categorical_features\n",
    "            )\n",
    "            \n",
    "            print(f\"Train pool size: {train_pool.shape}\")\n",
    "            print(f\"Val pool size: {val_pool.shape}\")\n",
    "            \n",
    "            # 8. Обучение модели\n",
    "            print(\"\\n8. Обучение CatBoostRanker...\")\n",
    "            \n",
    "            self.model = CatBoostRanker(**self.config.get_cb_params())\n",
    "            self.model.fit(train_pool, eval_set=val_pool)\n",
    "            \n",
    "            print(\"Обучение завершено!\")\n",
    "            \n",
    "            # 9. Сохранение модели\n",
    "            print(\"\\n9. Сохранение модели...\")\n",
    "            model_path = self.config.MODEL_DIR / \"catboost_ranker.cbm\"\n",
    "            self.model.save_model(str(model_path))\n",
    "            print(f\"Модель сохранена: {model_path}\")\n",
    "            \n",
    "            return self.model\n",
    "        \n",
    "        else:\n",
    "            print(\"\\nРежим предсказания: пропускаем обучение...\")\n",
    "            \n",
    "            # Загружаем сохраненную модель\n",
    "            model_path = self.config.MODEL_DIR / \"catboost_ranker.cbm\"\n",
    "            if model_path.exists():\n",
    "                print(f\"Загрузка модели из {model_path}...\")\n",
    "                self.model = CatBoostRanker()\n",
    "                self.model.load_model(str(model_path))\n",
    "                print(\"Модель загружена!\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Модель не найдена: {model_path}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def generate_submission(self):\n",
    "        \"\"\"Генерирует финальный сабмит\"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Данные не загружены. Сначала запустите run()\")\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала запустите run(train_mode=True)\")\n",
    "        \n",
    "        # Генерируем сабмит\n",
    "        submission_df = self.submission_generator.generate_submission(\n",
    "            self,\n",
    "            self.data['candidates'],\n",
    "            self.data['users'],\n",
    "            self.data['books'],\n",
    "            self.data['train'],\n",
    "            self.data.get('book_descriptions', pd.DataFrame()),\n",
    "            self.data.get('genres', pd.DataFrame()),\n",
    "            self.data.get('book_genres', pd.DataFrame())\n",
    "        )\n",
    "        \n",
    "        # Сохраняем сабмит\n",
    "        self.submission_generator.save_submission(submission_df)\n",
    "        \n",
    "        return submission_df\n",
    "\n",
    "\n",
    "# ============ АЛЬТЕРНАТИВНЫЙ ПРОСТОЙ МЕТОД ============\n",
    "\n",
    "def create_simple_submission(candidates_file: str, output_file: str = \"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Создает простой сабмит на основе исходных кандидатов\n",
    "    (если модель не работает, можно использовать этот метод для создания базового сабмита)\n",
    "    \n",
    "    Args:\n",
    "        candidates_file: Путь к файлу candidates.csv\n",
    "        output_file: Имя выходного файла\n",
    "    \"\"\"\n",
    "    print(\"Создание простого сабмита...\")\n",
    "    \n",
    "    # Загружаем candidates\n",
    "    candidates = pd.read_csv(candidates_file)\n",
    "    \n",
    "    submission_list = []\n",
    "    \n",
    "    for _, row in candidates.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        book_id_list = row['book_id_list']\n",
    "        \n",
    "        # Разделяем строку на book_id\n",
    "        book_ids = [bid.strip() for bid in str(book_id_list).split(',')]\n",
    "        \n",
    "        # Удаляем дубликаты\n",
    "        seen = set()\n",
    "        unique_book_ids = []\n",
    "        for bid in book_ids:\n",
    "            if bid and bid not in seen:\n",
    "                unique_book_ids.append(bid)\n",
    "                seen.add(bid)\n",
    "        \n",
    "        # Берем топ-20 или меньше\n",
    "        top_k = min(20, len(unique_book_ids))\n",
    "        top_books = unique_book_ids[:top_k]\n",
    "        \n",
    "        # Формируем строку\n",
    "        book_id_str = ','.join(top_books)\n",
    "        submission_list.append([user_id, book_id_str])\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    submission_df = pd.DataFrame(submission_list, columns=['user_id', 'book_id_list'])\n",
    "    \n",
    "    # Сохраняем\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Сабмит сохранен: {output_file}\")\n",
    "    print(f\"Количество пользователей: {len(submission_df)}\")\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "# ============ ПОЛНЫЙ ПАЙПЛАЙН С САБМИТОМ ============\n",
    "\n",
    "def full_pipeline():\n",
    "    \"\"\"Полный пайплайн: обучение + создание сабмита\"\"\"\n",
    "    # Инициализация\n",
    "    seed_everything(Config.RANDOM_STATE)\n",
    "    \n",
    "    # Создание пайплайна\n",
    "    pipeline = RankingPipeline(Config())\n",
    "    \n",
    "    try:\n",
    "        # 1. Обучение модели\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ЭТАП 1: ОБУЧЕНИЕ МОДЕЛИ\")\n",
    "        print(\"=\"*50)\n",
    "        model = pipeline.run(train_mode=True)\n",
    "        \n",
    "        # 2. Создание сабмита\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ЭТАП 2: ФОРМИРОВАНИЕ САБМИТА\")\n",
    "        print(\"=\"*50)\n",
    "        submission = pipeline.generate_submission()\n",
    "        \n",
    "        # 3. Проверка формата сабмита\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ЭТАП 3: ПРОВЕРКА ФОРМАТА САБМИТА\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Пример вывода первых строк сабмита\n",
    "        print(\"\\nПервые 5 строк сабмита:\")\n",
    "        print(submission.head())\n",
    "        \n",
    "        # Проверка количества книг на пользователя\n",
    "        book_counts = submission['book_id_list'].apply(lambda x: len(x.split(',')) if x else 0)\n",
    "        print(f\"\\nСтатистика по количеству книг на пользователя:\")\n",
    "        print(f\"  - Минимум: {book_counts.min()}\")\n",
    "        print(f\"  - Максимум: {book_counts.max()}\")\n",
    "        print(f\"  - Среднее: {book_counts.mean():.1f}\")\n",
    "        print(f\"  - Медиана: {book_counts.median():.1f}\")\n",
    "        \n",
    "        # Проверка уникальности user_id\n",
    "        if submission['user_id'].nunique() == len(submission):\n",
    "            print(f\"\\n✓ Все user_id уникальны ({len(submission)} пользователей)\")\n",
    "        else:\n",
    "            print(f\"\\n✗ Есть дубликаты user_id\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ПАЙПЛАЙН УСПЕШНО ЗАВЕРШЕН!\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        return model, submission\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при выполнении пайплайна: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# ============ АЛЬТЕРНАТИВНЫЙ ПАЙПЛАЙН (ТОЛЬКО САБМИТ) ============\n",
    "\n",
    "def submission_only_pipeline():\n",
    "    \"\"\"Пайплайн только для создания сабмита (если модель уже обучена)\"\"\"\n",
    "    # Инициализация\n",
    "    seed_everything(Config.RANDOM_STATE)\n",
    "    \n",
    "    # Создание пайплайна\n",
    "    pipeline = RankingPipeline(Config())\n",
    "    \n",
    "    try:\n",
    "        # 1. Загрузка данных\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ЭТАП 1: ЗАГРУЗКА ДАННЫХ\")\n",
    "        print(\"=\"*50)\n",
    "        pipeline.data = pipeline.data_loader.load_all_data()\n",
    "        \n",
    "        # 2. Загрузка модели\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ЭТАП 2: ЗАГРУЗКА МОДЕЛИ\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        model_path = Config.MODEL_DIR / \"catboost_ranker.cbm\"\n",
    "        if model_path.exists():\n",
    "            print(f\"Загрузка модели из {model_path}...\")\n",
    "            pipeline.model = CatBoostRanker()\n",
    "            pipeline.model.load_model(str(model_path))\n",
    "            print(\"Модель загружена!\")\n",
    "        else:\n",
    "            print(f\"Модель не найдена: {model_path}\")\n",
    "            print(\"Запускаем обучение модели...\")\n",
    "            pipeline.run(train_mode=True)\n",
    "        \n",
    "        # 3. Создание сабмита\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ЭТАП 3: ФОРМИРОВАНИЕ САБМИТА\")\n",
    "        print(\"=\"*50)\n",
    "        submission = pipeline.generate_submission()\n",
    "        \n",
    "        return submission\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при выполнении пайплайна: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============ ТОЧКА ВХОДА ============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Вариант 1: Полный пайплайн (обучение + сабмит)\n",
    "    print(\"Выберите режим работы:\")\n",
    "    print(\"1. Полный пайплайн (обучение + сабмит)\")\n",
    "    print(\"2. Только создание сабмита (если модель уже обучена)\")\n",
    "    print(\"3. Создать простой сабмит из candidates.csv (без модели)\")\n",
    "    \n",
    "    # try:\n",
    "    #     choice = int(input(\"Введите номер (1, 2 или 3): \"))\n",
    "    # except:\n",
    "    #     choice = 1\n",
    "    choice = 1\n",
    "    \n",
    "    if choice == 1:\n",
    "        model, submission = full_pipeline()\n",
    "    elif choice == 2:\n",
    "        submission = submission_only_pipeline()\n",
    "    elif choice == 3:\n",
    "        # Простой сабмит без модели\n",
    "        candidates_file = Config.DATA_DIR / \"candidates.csv\"\n",
    "        if not candidates_file.exists():\n",
    "            print(f\"Файл не найден: {candidates_file}\")\n",
    "            # Попробуем найти в другом месте\n",
    "            candidates_file = Path(\"/kaggle/input/nto-team-tour/public/candidates.csv\")\n",
    "        \n",
    "        submission = create_simple_submission(\n",
    "            candidates_file=str(candidates_file),\n",
    "            output_file=str(Config.RESULTS_DIR / \"simple_submission.csv\")\n",
    "        )\n",
    "    else:\n",
    "        print(\"Неверный выбор, запускаем полный пайплайн...\")\n",
    "        model, submission = full_pipeline()\n",
    "    \n",
    "    if submission is not None:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"САБМИТ ГОТОВ!\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Файл: {Config.RESULTS_DIR / 'submission.csv'}\")\n",
    "        print(\"\\nПример формата сабмита:\")\n",
    "        print(submission.head(3).to_string())\n",
    "        \n",
    "        # Сохраняем также в формате для проверки\n",
    "        sample_path = Config.RESULTS_DIR / \"submission_sample.txt\"\n",
    "        with open(sample_path, 'w') as f:\n",
    "            f.write(submission.head(10).to_string())\n",
    "        print(f\"\\nПример сохранен в: {sample_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
